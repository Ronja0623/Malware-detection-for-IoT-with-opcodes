import os
import r2pipe
import pandas as pd
from nltk.util import ngrams
from collections import Counter
from multiprocessing import Pool, cpu_count

def get_file_list(dir_path):
    return os.listdir(dir_path)

def get_file_path(dir_path, file_name):
    return os.path.join(dir_path, file_name)

def disam(input_file_path, output_json_path):
    r2 = r2pipe.open(input_file_path)
    r2.cmd("aa")
    r2.cmd("pdaj > {}".format(output_json_path))

def convert_to_csv(input_json_path, output_csv_path):
    df = pd.read_json(input_json_path)
    df_split = df['inst'].str.split(expand=True)
    df = pd.concat([df, df_split], axis=1)
    df.rename(columns = {0: 'opcode'}, inplace=True)
    df.drop(['bytes', 'inst', 1, 2, 3], axis=1, inplace=True)
    df.to_csv(output_csv_path)

def preprocess(file_name):
    input_file_path = get_file_path(directory_path, file_name)
    output_json_path = './01_json/{}.json'.format(file_name)
    output_csv_path = './02_csv/{}.json'.format(file_name)
    disam(input_file_path, output_json_path)
    convert_to_csv(output_json_path, output_csv_path)

def get_ngrams(df, n):
    return Counter(ngrams(df['opcode'], n))

if __name__ == "__main__":
    directory_path = './dataset'
    cpu_num = cpu_count()
    pool = Pool(cpu_num)
    file_list = get_file_list(directory_path)
    # TODO: renew the progress
    for i in file_list:
        input_pool = []
        for j in range(cpu_num):
            input_pool.append(i)
        pool.map(preprocess, input_pool)