import os

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
)
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm

from utils.npy_folder import NpyFolder


class DataProcessor:
    """
    Process the dataset and load it into PyTorch DataLoader.
    """

    def __init__(self, dataset_dir):
        self.dataset_dir = dataset_dir

    def get_transform(self, model_image_size):
        """
        Get the transformation for the dataset.
        """
        return transforms.Compose(
            [
                transforms.ToPILImage(),
                transforms.Resize(model_image_size),
                transforms.ToTensor(),
            ]
        )

    def get_num_classes(self):
        """
        Get the number of classes in the dataset.
        """
        return len(os.listdir(self.dataset_dir))

    def load_data(
        self,
        batch_size,
        model,
        data_description_path,
        train_ratio=0.7,
        val_ratio=0.2,
    ):
        """
        Load the dataset into PyTorch DataLoader.
        """
        if hasattr(model, "image_size"):
            transform = self.get_transform(model.image_size)
        else:
            transform = None
        dataset = dataset = NpyFolder(
            csv_file=data_description_path,
            root_dir=self.dataset_dir,
            transform=transform,
        )
        train_size = int(train_ratio * len(dataset))
        val_test_size = len(dataset) - train_size
        val_size = int(val_ratio * len(dataset))
        test_size = val_test_size - val_size
        train_data, val_data, test_data = random_split(
            dataset, [train_size, val_size, test_size]
        )
        train_loader = DataLoader(
            train_data, batch_size=batch_size, shuffle=True
        )
        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)
        return train_loader, val_loader, test_loader


class ModelTrainer:
    """
    Train and evaluate the model.
    """

    def __init__(self, model, metrics_manager, device, learning_rate, epochs):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()
        self.epochs = epochs
        self.metrics_manager = metrics_manager

    def train(self, train_loader, val_loader, model_dir):
        """
        Train the model.

        # TODO epoch should be set at model not here
        """
        for epoch in range(self.epochs):
            self.model.train()
            train_losses = []
            all_preds, all_labels = [], []
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}")
            for inputs, labels in progress_bar:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
                train_losses.append(loss.item())
                _, preds = torch.max(outputs, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                progress_bar.set_postfix(loss=loss.item())
            # Save the model
            model_path = os.path.join(model_dir, f"model_{epoch + 1}.pth")
            torch.save(self.model.state_dict(), model_path)
            # Calculate training metrics
            train_accuracy = accuracy_score(all_labels, all_preds)
            train_precision = precision_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            train_recall = recall_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            train_f1 = f1_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            average_loss = sum(train_losses) / len(train_losses)
            # Record training metrics
            self.metrics_manager.record_metrics(
                epoch + 1,
                average_loss,
                train_accuracy,
                train_precision,
                train_recall,
                train_f1,
                "train",
            )

            # Evaluate on validation set
            self.evaluate(val_loader, epoch)

    def evaluate(self, val_loader, epoch):
        """
        Evaluate the model on the validation set.
        """
        self.model.eval()
        val_losses = []
        all_preds, all_labels = [], []
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                val_losses.append(loss.item())
                _, preds = torch.max(outputs.data, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

            val_accuracy = accuracy_score(all_labels, all_preds)
            val_precision = precision_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            val_recall = recall_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            val_f1 = f1_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            average_loss = sum(val_losses) / len(val_losses)

            # Record validation metrics
            self.metrics_manager.record_metrics(
                epoch + 1,
                average_loss,
                val_accuracy,
                val_precision,
                val_recall,
                val_f1,
                "validate",
            )

    def test(self, test_loader):
        """
        Test the model on the test set.
        # TODO: add metrics manager to record test metrics
        """
        self.model.eval()
        test_losses = []
        all_preds, all_labels = [], []
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                test_losses.append(loss.item())
                _, preds = torch.max(outputs.data, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

            test_accuracy = accuracy_score(all_labels, all_preds)
            test_precision = precision_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            test_recall = recall_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            test_f1 = f1_score(
                all_labels, all_preds, average="macro", zero_division=0
            )
            average_loss = sum(test_losses) / len(test_losses)

            # Record test metrics
            self.metrics_manager.record_metrics(
                0,
                average_loss,
                test_accuracy,
                test_precision,
                test_recall,
                test_f1,
                "test",
            )
