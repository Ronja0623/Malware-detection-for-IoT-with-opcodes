import os
import numpy as np
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from utils import get_filename_with_time, get_filtered_list, get_ware_label, set_dataset_description

class Model():
    def __init__(self,
                 config,
                 df_dataset_discription,
                 model,
                 cpu_arc) -> None:
        self.config = config
        self.cpu_arc = cpu_arc
        set_dataset_description(df_dataset_discription)
        # path
        self.input_data_dir = os.path.join(self.config.folder.vectorize, 'eigen')
        self.output_report_dir = self.config.folder.report
        self.output_model_dir = self.config.folder.model
        # data
        self.x_data = []
        self.y_data = []
        self.name_data = []
        # hyperparameter
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = model.to(self.device)
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters())
    
    # TODO: analysis of the result if preprocess or not
    def get_data(self):
        ca_list = get_filtered_list('CPU Architecture', self.cpu_arc)
        for file_name in ca_list:
            file_path = os.path.join(self.input_data_dir, f'{file_name}.npy')
            if not os.path.exists(file_path):
                continue
            sample = np.load(file_path)
            self.x_data.append(sample)
            self.name_data.append(file_name)
            label = get_ware_label(file_name)
            self.y_data.append(label)
    
    def turn_data_to_np(self):
        self.x_data = np.array(self.x_data)
        self.y_data = [1 if label == 'malware' else 0 for label in self.y_data]
        self.y_data = np.array(self.y_data)
    
    def get_eigenvalue_distribution(self):
        first_eigenvalues = self.x_data[:, 0]
        second_eigenvalues = self.x_data[:, 1]
        plt.hist(first_eigenvalues, bins=10)
        plt.title('Distribution of the first eigenvalues')
        plt.savefig(os.path.join(self.config.folder.report,
                                 get_filename_with_time(f'first_eigenvalues_distribution_{self.cpu_arc}',
                                           '.png')))
        plt.clf()
        plt.hist(second_eigenvalues, bins=10)
        plt.title('Distribution of the second eigenvalues')
        plt.savefig(os.path.join(self.config.folder.report,
                                 get_filename_with_time(f'second_eigenvalues_distribution_{self.cpu_arc}',
                                           '.png')))
        plt.clf()
    
    # normalize first eigenvalue, second eigenvalue normalize
    def normalize_eigenvalues(self):
        eigenvalues = self.x_data[:, :2]
        min_val = np.min(eigenvalues, axis=0)
        max_val = np.max(eigenvalues, axis=0)
        normalized_eigenvalues = (eigenvalues - min_val) / (max_val - min_val)
        self.x_data[:, :2] = normalized_eigenvalues

    def devide_data(self):
        x_tmp, self.x_test, y_tmp, self.y_test, name_tmp, self.name_test = train_test_split(self.x_data, self.y_data, self.name_data, test_size=0.2, random_state=42)
        if len(x_tmp) > 1:
            self.x_train, self.x_val, self.y_train, self.y_val, self.name_train, self.name_val = train_test_split(x_tmp, y_tmp, name_tmp, test_size=1/9, random_state=42)
        else:
            print("Not enough samples for train/test split.")
            return False

    def preprocess(self, isNormalize):
        self.get_data()
        print(f'Number of data: {len(self.x_data)}')
        if len(self.x_data) == 0:
            return False
        self.turn_data_to_np()
        self.get_eigenvalue_distribution()
        if isNormalize:
            self.normalize_eigenvalues
        if self.devide_data() == False:
            return False
        return True

    def load_data(self):
        self.x_train = torch.from_numpy(self.x_train).float()
        self.y_train = torch.from_numpy(self.y_train).float()
        self.x_val = torch.from_numpy(self.x_val).float()
        self.y_val = torch.from_numpy(self.y_val).float()
        self.x_test = torch.from_numpy(self.x_test).float()
        self.y_test = torch.from_numpy(self.y_test).float()

    def train(self):
        self.model.train()
        data_loader = DataLoader(list(zip(self.x_train, self.y_train)), batch_size=32, shuffle=True)
        for inputs, targets in data_loader:
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets.long())
            loss.backward()
            self.optimizer.step()
    
    def validate(self):
        self.model.eval()
        all_targets = []
        all_predictions = []
        with torch.no_grad():
            data_loader = DataLoader(list(zip(self.x_val, self.y_val)), batch_size=32, shuffle=True)
            for inputs, targets in data_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = self.model(inputs)
                _, predicted = torch.max(outputs, 1)
                all_targets.extend(targets.cpu().numpy())
                all_predictions.extend(predicted.cpu().numpy())
        accuracy = accuracy_score(all_targets, all_predictions)
        precision = precision_score(all_targets, all_predictions, average='macro', zero_division=0)
        recall = recall_score(all_targets, all_predictions, average='macro', zero_division=0)
        f_measure = f1_score(all_targets, all_predictions, average='macro')
        return accuracy, precision, recall, f_measure

    def fit(self, epochs):
        results = []
        for epoch in range(epochs):
            self.train()
            accuracy, precision, recall, f_measure = self.validate()
            results.append([epoch + 1, precision, recall, f_measure, accuracy])
        # save model
        torch.save(self.model.state_dict(), os.path.join(self.output_model_dir, get_filename_with_time(f'model_{self.cpu_arc}', '.pth')))
        # save results
        with open(os.path.join(self.config.folder.report,
                               get_filename_with_time(f'model_performance_{self.cpu_arc}', '.csv')), 'w') as f:
            f.write('Epoch,Accuracy,Precision,Recall,F-measure\n')
            for result in results:
                f.write(','.join(map(str, result)) + '\n')
    
    def predict(self):
        self.model.eval()
        all_predictions = []
        with torch.no_grad():
            for inputs, name in zip(self.x_test, self.name_test):
                inputs = inputs.to(self.device)
                outputs = self.model(inputs)
                _, predicted = torch.max(outputs, 1)
                all_predictions.append((name, predicted.cpu().item()))
        with open(os.path.join(self.config.folder.predict,
                               get_filename_with_time(f'predictions_{self.cpu_arc}', '.csv')), 'w') as f:
            f.write('filename,prediction\n')
            for name, prediction in all_predictions:
                predict_str = 'malware' if prediction == 1 else 'benignware'
                f.write(f'{name},{predict_str}\n')

# class: basic model CNN
class BasicCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(32 * 1 * 12, 128)
        self.fc2 = nn.Linear(128, 2)
    
    def forward(self, x):
        n = x.shape[-1]
        x = x.unsqueeze(1).unsqueeze(1)
        x = x.view(-1, 1, 1, n)
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = x.view(-1, 32 * 1 * 12)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# class: machine unlearning model
# class: classification
