import csv
import os
from multiprocessing import Pool

from tqdm import tqdm

from .aggregate import Aggregate
from .shard import Shard
from .slice import Slice


class MachineUnlearning:
    def __init__(self, dataset_path, num_shard, num_slice, epochs, sisa_file):
        if num_shard < 1 or num_slice < 1:
            raise ValueError("num_shard and num_slice should be greater than 0")
        self.num_shard = num_shard
        self.num_slice = num_slice
        self.epochs = max(epochs, num_slice)
        self.dataset_path = dataset_path
        self.shard = Shard(self.num_shard)
        self.slice = Slice(self.num_slice)
        self.aggregate = Aggregate()
        self.sisa_file = sisa_file

    def init_model(self, train_method, dataset_path, model_dir):
        sharded_list = self.shard.divide_by_shard(dataset_path)
        sliced_list = [self.slice.divide_by_slice(sublist) for sublist in sharded_list]
        self.save_sisa_description(sliced_list, self.sisa_file)
        epochs_per_slice = self.epochs // len(sliced_list)
        trained_epochs = 0
        with Pool() as pool:
            results = []
            for i, sublist in enumerate(tqdm(sliced_list, desc="Training Shards")):
                for j, subsublist in enumerate(sublist):
                    epoch_num = (
                        self.epochs - trained_epochs
                        if j == len(subsublist) - 1
                        else epochs_per_slice
                    )
                    result = pool.apply_async(
                        train_method,
                        args=(self.sisa_file, epoch_num, model_dir, i, j),
                    )
                    results.append(result)
                    trained_epochs += epoch_num
            [result.get() for result in results]

    def save_sisa_description(self, sliced_list, output_file):
        with open(output_file, "w", newline="") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(["data_file", "shard_index", "slice_index"])
            for i, sublist in enumerate(sliced_list):
                for j, subsublist in enumerate(sublist):
                    for data_file in subsublist:
                        writer.writerow([data_file, i, j])

    def load_sisa_description(self, input_file):
        sliced_list = []
        with open(input_file, "r", newline="") as csvfile:
            reader = csv.reader(csvfile)
            next(reader)  # Skip header row
            for row in reader:
                shard_index = int(row[1])
                slice_index = int(row[2])
                while len(sliced_list) <= shard_index:
                    sliced_list.append([])
                while len(sliced_list[shard_index]) <= slice_index:
                    sliced_list[shard_index].append([])
                sliced_list[shard_index][slice_index].append(row[0])
        return sliced_list

    def add_data_point(self, train_method, add_data_files, dataset_path, model_dir):
        sliced_list = self.load_sisa_description(self.sisa_file)
        new_sharded_list, updated_shard_indices = self.shard.distribute_files(
            sliced_list, add_data_files
        )
        result = [
            self.slice.append_files(sublist, add_data_files)
            for sublist in new_sharded_list
        ]
        new_sliced_list, updated_slice_indices = zip(*result)
        self.save_sisa_description(new_sliced_list, self.sisa_file)
        epochs_per_slice = self.epochs // len(new_sliced_list)
        trained_epochs = 0
        with Pool() as pool:
            results = []
            for i, sublist in enumerate(
                tqdm(new_sliced_list, desc="Retraining Shards")
            ):
                for j, subsublist in enumerate(sublist):
                    epoch_num = (
                        self.epochs - trained_epochs
                        if j == len(subsublist) - 1
                        else epochs_per_slice
                    )
                    result = pool.apply_async(
                        train_method,
                        args=(self.sisa_file, epoch_num, model_dir, i, j),
                    )
                    results.append(result)
                    trained_epochs += epoch_num
            [result.get() for result in results]

    def delete_data_point(
        self, train_method, delete_data_files, dataset_path, model_dir
    ):
        sliced_list = self.load_sisa_description(self.sisa_file)
        for data_file in delete_data_files:
            for sublist in sliced_list:
                for subsublist in sublist:
                    if data_file in subsublist:
                        subsublist.remove(data_file)
        self.save_sisa_description(sliced_list, self.sisa_file)
        self.init_model(train_method, dataset_path, model_dir)

    def predict(self, model_dir, predict_data_files, predict_method):
        model_path_list = self.aggregate.get_model_path_list(model_dir)
        predictions = []
        for model_path in model_path_list:
            prediction = predict_method(predict_data_files, model_path)
            predictions.append(prediction)
        final_prediction = self.aggregate.vote(predictions)
        return final_prediction
